---
title: 懒猫算力仓初探（一）：开箱手记
tags: NAS
toc: true
categories: 懒猫微服
date: 2025-10-26 00:00:00
---

懒猫微服最近上新了一款产品，名字叫 **「懒猫 AI 算力仓」**。
作为老用户，首发自然第一时间支持了一下。

其实我对这个设备的期待已经很久了。过去一年里，AI 大模型的爆发几乎让每个技术人都产生过同一个念头：能不能有一台属于自己的“算力仓”？不用再排队租云 GPU，不用担心账单像无底洞一样增长，更不用把敏感数据传到云端。所以赶了个首发，等了两天拿到了商品。

**懒猫 AI 算力仓的核心是 NVIDIA Jetson AGX Orin**。

<!-- more -->

这块板子可能很多人都听过，它本来的定位是“边缘计算”和“机器人中枢”，算是英伟达给嵌入式 AI 场景设计的亲儿子产品。比如跑 YOLO 目标检测、机械臂控制，这些都是它的常见场景。

除此之外， Orin 还有一个优势，就是 **完整继承 CUDA 生态**。这就意味着，它和桌面 GPU 一样，能跑主流大模型和 AI 框架。相比之下，很多国产芯片虽然跑分也许很高，但因为缺乏 CUDA 生态，真正落地的时候往往要做大量适配工作。可能你下载了模型，但能不能跑起来就是另一个问题了。

![渲染图](https://raw.githubusercontent.com/cloudsmithy/picgo-imh/master/image-20250926190036366-20250926191346936-20250926191839278-20250926192032634.png)

**CUDA = 生态优势**。

这点在 AI 时代非常关键。使用懒猫 AI 算力仓，下载完模型就能马上测试，而非 CUDA 芯片用户，可能还要花上几个月适配环境。这就是差距。

所以当懒猫把 Orin 改造成家用级的 AI 算力仓时，我的第一反应就是：**这玩意儿正好补上了家里缺的那块“私有超算”拼图。**

包装比我预想的更精美，拆开的时候甚至有点“科幻感”。外观是简洁的机身，棱角分明，风格上有点像星球大战里的装置。

![实物图](https://raw.githubusercontent.com/cloudsmithy/picgo-imh/master/eb751b92c3b4aea950adf5df8818fef2-20250926191424716-20250926192101025.jpg)

尽管实物拿着沉甸甸的，但尺寸比我想象的还要小一些，高度大约只有懒猫微服的 NAS 一半。拿在手里的重量也不到 1300 克，比 MacBook Pro 还要轻很多，所以理论上你带出门也不是不行（不是）。

![重量图](https://raw.githubusercontent.com/cloudsmithy/picgo-imh/master/a3ae36dd200008b4f2d4b97e3828aca2-20250926191540512-20250926191840322-20250926192111914.jpg)

这种“袖珍 + 全固态”的组合，第一观感就是结实耐用，而且空间利用率极高。塞进机柜里刚刚好。

接下来看看核心参数：

- **GPU**：275T 算力，64 Tensor Core，2048 核心，最高 1.3GHz
- **CPU**：12 核 ARM Cortex-A78AE，最高 2.0GHz，轻量推理和任务调度非常合适
- **显存**：LPDDR5，带宽 204.8GB/s，容量高达 64GB（显存+内存一体化设计）

这套组合，基本就决定了它能直接运行 **70B 参数级别的大模型**，而且是真正意义上的“无限 Tokens”，不像云端那样有额度限制。

在存储扩展方面：

- 自带 64GB eMMC 5.1 系统盘（足够跑系统和基础应用）
- 预留 2 个 M.2 插槽，最多支持 **32TB SSD**，这就意味着即便你想同时存储多个大模型，也能一次性装下。

接口配置也很周到：

- 视频输出：HDMI
- 外设：2 个 USB 3.2
- 网络：双网口（10G + 2.5G），特别适合多机联动或做本地集群
- 额外：还支持 WiFi6，无线连接没问题

![接口图](https://raw.githubusercontent.com/cloudsmithy/picgo-imh/master/screen2_pc_2-20250926191320702-20250926191839972.png)

官方参数写着整机功耗 **64W**，所以很省电。要知道，一块台式机的 4090 往往就能拉到 400W，而 Orin 的 64W 几乎可以说是“节能怪兽”。我把一个小型工作站，直接塞进了我的机柜里。

这次买算力仓，还有一个重要原因：我家里已经在用懒猫微服的生态。算力仓一接入，就能和原来的设备形成闭环。

比如：

- NAS 负责存储数据和模型；
- 算力仓负责推理和生成；
- 懒猫微服的软件层负责调度和调用。

这种组合让我感觉像是给自己搭了一个“家用 AI 实验室”。来一张和机柜里其他设备的合影 ↓

![机柜合影](https://raw.githubusercontent.com/cloudsmithy/picgo-imh/master/4a69c38d0f8bae1668fb6ad5b7063909.jpg)

懒猫算力仓的应用场景非常多：

1. **大模型推理**：本地跑 70B 模型，直接当私有 ChatGPT，避免云端隐私风险。
2. **文生图 / 文生视频**：跑 Stable Diffusion、Luma AI 之类的模型，图生视频速度应该会很快。
3. **科研实验**：对于学生和研究者来说，算力仓相当于一台低功耗的工作站，可以在寝室里跑实验。
4. **企业内部 POC**：小公司如果要测试 AI 原型，直接上算力仓，比租云 GPU 灵活得多。
5. **边缘推理 + 本地 RAG**：结合家里的数据仓库，跑一个完全离线的知识问答系统，做“自己的 AI 知识助手”。

整体来说，**懒猫 AI 算力仓**给我的感觉就是：

- 小小一台，参数硬核；
- CUDA 生态无缝兼容，开箱即跑大模型；
- 扩展灵活，功耗低，噪音小；
- 和懒猫微服生态结合，几乎是“家用 AI 实验室”的标配；
- 对我这种经常要跑 RAG、文生图的场景非常友好。

如果说之前还在犹豫“要不要上云 GPU”，现在真的可以考虑直接在家里搞一台 Orin 算力仓。**算力、省钱、数据安全，全部兼顾。**

更重要的是，这不是一台冰冷的机器，而是让我随时能打开 AI 世界大门的“钥匙”。
